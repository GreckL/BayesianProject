MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem#sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr(df_pooled, model="rubin", pooling = "partial")
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-1)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
# Plot the MSEs to compare
df <- data.frame(x=1:nstud, MLE_MSE, MSE_BAY)
df_long <- df %>%
pivot_longer(cols = c(MLE_MSE, MSE_BAY), names_to = "Series", values_to = "Value")
# Plot with ggplot2
p <- ggplot(df_long, aes(x = x, y = Value, color = Series, group = Series)) +
geom_line(size = 1.2) +  # Add lines
geom_point(size = 3) +   # Add points
geom_text(aes(label = round(Value, 1)), vjust = -1, size = 3.5) +  # Add labels
theme_minimal() +        # Use a minimal theme
theme(
text = element_text(family = "Arial", size = 12),
plot.title = element_text(size = 16, face = "bold"),
legend.title = element_blank()
) +
labs(
title = "MSE Comparison in a Random Effects Setting",
x = "Number of Studies (n)",
y = "MSE"
)
# Display the plot
x11()
print(p)
View(bg)
warnings()
bg[["pooling_metric"]]
bg[["data"]][["tau"]]
View(theta_k_rows)
print(theta_MSE)
print(theta_k_rows)
print(theta_k_rows$mean)
print(theta_MSE)
theta_k_rows$mean-1
theta_k_rows$mean-theta_context
s=1         # Number of simulations
nstud = 40   # Maximum number of studies
theta_simul <- c()  #Simulate data set
theta_context <- c() #Simulate data set !NEW! Varying "true" ATE for each observation
sigma_simul <- c()  #Simulate data set
MLE_est <- c()    #Vector of simulated Frequentist weighted-pool Estimators
MLE_mean <- c()   #Monte Carlo average
MLE_MSE <- c()    #Mean Squared Error of Frequentist approach
BAY_MSE <- c()    #Mean Squared Error of Bayesian approach
bg <- c()     #Bayesian Model
BAY_tau <- c()
bg_mse <- c()   #Bayesian Mean Squared Error
MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem#sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr(df_pooled, model="rubin", pooling = "none")
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-theta_context)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
# Plot the MSEs to compare
df <- data.frame(x=1:nstud, MLE_MSE, MSE_BAY)
df_long <- df %>%
pivot_longer(cols = c(MLE_MSE, MSE_BAY), names_to = "Series", values_to = "Value")
# Plot with ggplot2
p <- ggplot(df_long, aes(x = x, y = Value, color = Series, group = Series)) +
geom_line(size = 1.2) +  # Add lines
geom_point(size = 3) +   # Add points
geom_text(aes(label = round(Value, 1)), vjust = -1, size = 3.5) +  # Add labels
theme_minimal() +        # Use a minimal theme
theme(
text = element_text(family = "Arial", size = 12),
plot.title = element_text(size = 16, face = "bold"),
legend.title = element_blank()
) +
labs(
title = "MSE Comparison in a Random Effects Setting",
x = "Number of Studies (n)",
y = "MSE"
)
# Display the plot
x11()
print(p)
print(MSE_BAY)
print(theta_k_rows)
vignette(baggr)
vignette("baggr")
print(bg)
print(theta_k_rows)
s=1         # Number of simulations
nstud = 40   # Maximum number of studies
theta_simul <- c()  #Simulate data set
theta_context <- c() #Simulate data set !NEW! Varying "true" ATE for each observation
sigma_simul <- c()  #Simulate data set
MLE_est <- c()    #Vector of simulated Frequentist weighted-pool Estimators
MLE_mean <- c()   #Monte Carlo average
MLE_MSE <- c()    #Mean Squared Error of Frequentist approach
BAY_MSE <- c()    #Mean Squared Error of Bayesian approach
bg <- c()     #Bayesian Model
BAY_tau <- c()
bg_mse <- c()   #Bayesian Mean Squared Error
MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem#sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr(df_pooled, model="rubin", pooling = "partial",iter=3000, chains=2)
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-theta_context)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
# Plot the MSEs to compare
df <- data.frame(x=1:nstud, MLE_MSE, MSE_BAY)
df_long <- df %>%
pivot_longer(cols = c(MLE_MSE, MSE_BAY), names_to = "Series", values_to = "Value")
# Plot with ggplot2
p <- ggplot(df_long, aes(x = x, y = Value, color = Series, group = Series)) +
geom_line(size = 1.2) +  # Add lines
geom_point(size = 3) +   # Add points
geom_text(aes(label = round(Value, 1)), vjust = -1, size = 3.5) +  # Add labels
theme_minimal() +        # Use a minimal theme
theme(
text = element_text(family = "Arial", size = 12),
plot.title = element_text(size = 16, face = "bold"),
legend.title = element_blank()
) +
labs(
title = "MSE Comparison in a Random Effects Setting",
x = "Number of Studies (n)",
y = "MSE"
)
# Display the plot
x11()
print(p)
effect_plot(bg)
print(theta_k_rows)
print(p)
vignette(("baggr"))
forest_plot(bg)
forest_plot(bg)
forest_plot(bg)
vignette(("baggr"))
s=1         # Number of simulations
nstud = 20   # Maximum number of studies
theta_simul <- c()  #Simulate data set
theta_context <- c() #Simulate data set !NEW! Varying "true" ATE for each observation
sigma_simul <- c()  #Simulate data set
MLE_est <- c()    #Vector of simulated Frequentist weighted-pool Estimators
MLE_mean <- c()   #Monte Carlo average
MLE_MSE <- c()    #Mean Squared Error of Frequentist approach
BAY_MSE <- c()    #Mean Squared Error of Bayesian approach
bg <- c()     #Bayesian Model
BAY_tau <- c()
bg_mse <- c()   #Bayesian Mean Squared Error
MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem #sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr_compare(df_pooled, model="rubin",iter=2000, chains=1)
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-theta_context)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
effect_plot(bg)
plot(my_baggr_comparison) +
ggtitle("8 schools: model comparison")
View(bg)
s=1         # Number of simulations
nstud = 20   # Maximum number of studies
theta_simul <- c()  #Simulate data set
theta_context <- c() #Simulate data set !NEW! Varying "true" ATE for each observation
sigma_simul <- c()  #Simulate data set
MLE_est <- c()    #Vector of simulated Frequentist weighted-pool Estimators
MLE_mean <- c()   #Monte Carlo average
MLE_MSE <- c()    #Mean Squared Error of Frequentist approach
BAY_MSE <- c()    #Mean Squared Error of Bayesian approach
bg <- c()     #Bayesian Model
BAY_tau <- c()
bg_mse <- c()   #Bayesian Mean Squared Error
MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem #sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr_compare(df_pooled, model="rubin",iter=3000, chains=1)
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-theta_context)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
s=1         # Number of simulations
nstud = 20   # Maximum number of studies
theta_simul <- c()  #Simulate data set
theta_context <- c() #Simulate data set !NEW! Varying "true" ATE for each observation
sigma_simul <- c()  #Simulate data set
MLE_est <- c()    #Vector of simulated Frequentist weighted-pool Estimators
MLE_mean <- c()   #Monte Carlo average
MLE_MSE <- c()    #Mean Squared Error of Frequentist approach
BAY_MSE <- c()    #Mean Squared Error of Bayesian approach
bg <- c()     #Bayesian Model
BAY_tau <- c()
bg_mse <- c()   #Bayesian Mean Squared Error
MSE_hat <- 0
MSE_BAY <- c()
theta_true1 <- 1   #True Fixed Treatment Effect Group 1
theta_true2 <- 30  #True Fixed Treatment Effect Group 2
for (n in 2:nstud) {    # Number of studies (meta analysis so start from 2 up to nstud)
for (i in 1:s) {      # Number of simulations
set.seed(s)
for (t in 1:n) {    # Generator process (study-dependent)
group_indicator <- sample(c(1, 2), 1)  # Randomly assign each t to a group
if (group_indicator == 1) {
theta_context[t] <- theta_true1
} else {
theta_context[t] <- theta_true2
}
sigma_simul[t] = runif(1,min = 0.1,max = 0.5) #Variance
theta_simul[t] = rnorm(1,mean = theta_context[t],sd = sigma_simul[t]) #ATE
}
# Frequentist
weighted_num = sum(theta_simul %*% sigma_simul)
weighted_dem = sum(sigma_simul)
MLE_est[i]<- weighted_num/weighted_dem #sum(theta_simul)
# Bayesian
df_pooled = data.frame("tau"= theta_simul,
"se" = sigma_simul)
## Using partial pooling from Bayesian bagger
bg <- baggr(df_pooled, model="rubin", pooling="partial", iter=2000, chains=1)
# Extract the Stan fit object
fit_object <- bg$fit
# Get the summary table from the Stan fit object
fit_summary <- summary(fit_object)$summary
# Convert it to a data frame for easier manipulation
fit_summary_df <- as.data.frame(fit_summary)
# Get row names containing "theta_k"
theta_k_rows <- fit_summary_df[grep("theta_k", rownames(fit_summary)), ]
theta_MSE <- (theta_k_rows$mean-theta_context)^2
BAY_MSE <- sum(unlist(theta_MSE))/n
MSE_hat = MSE_hat*((s-1)/s) + BAY_MSE*(1/s)
#*BAY_Nstud_MSE[i] <- sum(BAY_MSE)/n
# Print the theta_k rows
#print(theta_k_rows)
}
MLE_mean[n] <- sum(MLE_est)/s
MLE_MSE[n] <- (MLE_mean[n]-theta_true)^2
# bayesian
MSE_BAY[n] = MSE_hat
#tau_columns <- lapply(bg, function(df) df[["tau"]])
#BAY <- ba
#BAY_MSE[n] <- ((sum(unlist(tau_columns))/n)/s-theta_true)^2
#BAY_tau[i] <- hypermean(bg[i], transform = NULL,
#                        interval = 0.95,
#                        message = FALSE,
#                        summary = TRUE)
#bg_mse$n = (tau_columns[[n]]-1)^2
#BAY_MSE[n] = sum(unlist(bg_mse))/n
}
# Plot the MSEs to compare
df <- data.frame(x=1:nstud, MLE_MSE, MSE_BAY)
df_long <- df %>%
pivot_longer(cols = c(MLE_MSE, MSE_BAY), names_to = "Series", values_to = "Value")
# Plot with ggplot2
p <- ggplot(df_long, aes(x = x, y = Value, color = Series, group = Series)) +
geom_line(size = 1.2) +  # Add lines
geom_point(size = 3) +   # Add points
geom_text(aes(label = round(Value, 1)), vjust = -1, size = 3.5) +  # Add labels
theme_minimal() +        # Use a minimal theme
theme(
text = element_text(family = "Arial", size = 12),
plot.title = element_text(size = 16, face = "bold"),
legend.title = element_blank()
) +
labs(
title = "MSE Comparison in a Random Effects Setting",
x = "Number of Studies (n)",
y = "MSE"
)
# Display the plot
x11()
print(p)
effect_plot(bg)
